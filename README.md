# civic-tech-airflow

## Airflow Environment with Celery Workers and PostgreSQL Backend

This repository contains a fully functional Apache Airflow environment configured with Celery workers and a PostgreSQL database backend.

The setup uses Docker Compose to orchestrate services, including two Celery workers, a Redis broker, and a Postgres database.

---

## Table of Contents

1. [Introduction](#1-introduction)
2. [Features](#2-features)
3. [Prerequisites](#3-prerequisites)
4. [Start the Environment](#4-setup-and-start-the-environment)
5. [Usage](#5-usage)
    - [Accessing the Web UI](#accessing-the-web-ui)
    - [Triggering a DAG](#triggering-a-dag)
    - [Monitoring Tasks](#monitoring-tasks)
6. [Folder Structure](#6-folder-structure)
7. [Useful Commands](#7-useful-commands)
    - [Managing Airflow Services](#managing-airflow-services)
    - [Managing DAGs](#managing-dags)
    - [Managing Tasks](#managing-tasks)
    - [Database and Connections](#database-and-connections)
    - [Other Userful Commands](#other-useful-commands)
8. [License](#8-license)

---

## 1. Introduction

Apache Airflow is an open-source platform to programmatically author, schedule, and monitor workflows. This repository provides an Airflow setup using the Celery executor for distributed task execution and a PostgreSQL database as the metadata store.

The overall idea is to have a centralise orchestrator used to ingest open-data.

---

## 2. Features

- **Airflow 2.9.2** with the Celery Executor.
- **PostgreSQL 13** as the metadata database.
- **Redis** as the Celery message broker.
- Automatic loading of environment variables from `.env` files.
- Volume-mounted directories for DAGs, logs, and plugins.
- Multiple Celery workers for parallel task execution.
- Docker Compose for easy orchestration.

---

## 3. Prerequisites

Ensure the following are installed on your system:
- Docker (>= 20.10)
- Docker Compose (>= 2.0)
- Makefile installed
- Python (for generating Fernet keys, optional)

---

## 4. Setup and Start the Environment

In order to spin up the environment we first need to build the Docker image of our Airflow environment.

```bash
make build-local
```

After the docker image has been built remove the `.template` from the `.env.template` file and inject all the required environment variables in there.

To spinup the instance of Airflow and start to execute the different ETL run:

```bash
make spinup-local
```

The Airflow instance should be slowly spinup.

## 5. Usage

### Accessing the Web UI

- Airflow UI: [http://localhost:8080](http://localhost:8080)
- Default credentials:
  - **Username**: `admin`
  - **Password**: `admin`

### Triggering a DAG

1. Place your DAG files in the `airflow/dags` directory.
2. Open the Airflow Web UI, enable your DAG, and trigger it manually.

### Monitoring Tasks

You can monitor DAG execution, view logs, and track task statuses in the Airflow UI.

---

## 6. Folder Structure

```plaintext
├── airflow/
│   ├── dags/                # Place your DAG files here
│   ├── logs/                # Logs generated by Airflow
│   ├── requirements.airflow.txt # Python dependencies for Airflow
├── docker-compose.yml       # Docker Compose configuration
├── .env                     # Environment variables for Airflow
```

## 7. Useful Commands

### Managing Airflow Services

- **Restart the environment**  
  To restart the entire environment, including rebuilding the containers, run:
  ```
  docker-compose down && docker-compose up --build
  ```

- **View logs for a specific service**  
  To view logs for the `airflow-webserver` service:
  ```
  docker-compose logs -f airflow-webserver
  ```
  Replace `airflow-webserver` with other services like `airflow-scheduler`, `airflow-worker-1`, or `airflow-worker-2` to see their respective logs.

---

### Managing DAGs

- **List all DAGs**  
  To list all available DAGs:
  ```
  docker-compose exec airflow-webserver airflow dags list
  ```

- **Trigger a DAG**  
  To trigger a specific DAG:
  ```
  docker-compose exec airflow-webserver airflow dags trigger <dag_id>
  ```

- **Pause a DAG**  
  To pause a specific DAG:
  ```
  docker-compose exec airflow-webserver airflow dags pause <dag_id>
  ```

- **Unpause a DAG**  
  To unpause a specific DAG:
  ```
  docker-compose exec airflow-webserver airflow dags unpause <dag_id>
  ```

---

### Managing Tasks

- **Trigger a Task**  
  To trigger a specific task within a DAG:
  ```
  docker-compose exec airflow-webserver airflow tasks run <dag_id> <task_id> <execution_date>
  ```

- **View Task Logs**  
  To view the logs for a specific task execution:
  ```
  docker-compose exec airflow-webserver airflow tasks logs <dag_id> <task_id> <execution_date>
  ```

- **Clear Task Instances**  
  To clear task instances (mark them as unrun):
  ```
  docker-compose exec airflow-webserver airflow tasks clear <dag_id> --task_regex <task_id> --start_date <start_date> --end_date <end_date>
  ```

---

### Database and Connections

- **Initialize the Database**  
  To initialize the Airflow database:
  ```
  docker-compose exec airflow-webserver airflow db init
  ```

- **Check Database Status**  
  To check the status of the Airflow database:
  ```
  docker-compose exec airflow-webserver airflow db check
  ```

- **List Connections**  
  To list all configured connections:
  ```
  docker-compose exec airflow-webserver airflow connections list
  ```

- **Add a New Connection**  
  To add a new connection:
  ```
  docker-compose exec airflow-webserver airflow connections add 'my_conn_id' --conn-type 'postgres' --conn-host 'localhost' --conn-login 'user' --conn-password 'password' --conn-schema 'database'
  ```

---

### Other Useful Commands

- **Enter the Webserver container**  
  To access the `airflow-webserver` container’s bash:
  ```
  docker-compose exec airflow-webserver bash
  ```

- **Check the status of all services**  
  To check the status of all running services:
  ```
  docker-compose ps
  ```

- **Stop all services**  
  To stop all running containers:
  ```
  docker-compose down
  ```

- **Bring up the environment without rebuilding**  
  To start the services again without rebuilding the containers:
  ```
  docker-compose up
  ```

## 8. License

This project is licensed under the terms of the **Apache License 2.0**.

### 8.1. Apache License 2.0

You may not use this file except in compliance with the License. You may obtain a copy of the License at:

  [Apache License 2.0 URL](http://www.apache.org/licenses/LICENSE-2.0)

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an **"AS IS" BASIS**, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.

### 8.2. Third-party Libraries

This project may use or include third-party libraries. Each of these libraries may be subject to their own license terms, and you should ensure compliance with those licenses when using or redistributing them.

- **Apache Airflow** is licensed under the [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0).
- **PostgreSQL** is licensed under the [PostgreSQL License](https://opensource.org/licenses/PostgreSQL).
- **Redis** is licensed under the [Redis Source Available License](https://opensource.org/licenses/Redislabs).

Please check the respective library documentation for full licensing details.

## Authors & Credits

**Matteo Osio** - *Initial work & maintenance* 
- GitHub: [@osiom](https://github.com/osiom)
- Linkedin: [matteo osio](https://www.linkedin.com/in/matteoosio/)
